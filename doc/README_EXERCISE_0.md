# Exercise 0 ‚Äì Asistente Agentic Simple con contexto por ficheros

## ¬øQu√© es este ejercicio?

En el Exercise 0 el objetivo no es construir un agente complejo, sino entender
c√≥mo funciona un **agente m√≠nimo** que responde preguntas usando informaci√≥n
cargada directamente en el prompt del modelo.

Es un primer contacto con:

* agentes LLM
* inyecci√≥n de contexto
* flujo frontend ‚Üí backend ‚Üí LLM
* limitaciones de este enfoque

---

## Qu√© hace el agente

El agente implementado (`hotel_simple_agent`) responde preguntas sobre hoteles
utilizando informaci√≥n que se carga desde ficheros locales.

No utiliza:

* RAG
* herramientas
* memoria
* razonamiento multi-step

Simplemente:

1. carga los datos
2. construye un prompt
3. env√≠a el prompt y la pregunta al LLM
4. devuelve la respuesta

---

## De d√≥nde salen los datos (esto lo prob√© a mano)

El agente intenta cargar los datos desde dos posibles rutas:

1. **Ruta local del servicio (Docker)**

   ```
   ai_agents_hospitality-api/data/hotels/
   ```

2. **Ruta externa (datos generados)**

   ```
   bookings-db/output_files/hotels/
   ```

Durante las pruebas comprob√© que, cuando existen datos en la carpeta `data/hotels`
del servicio, **el agente usa esos datos**, no los generados en `bookings-db`.

Esto se ve claramente en los logs:

```
Using local hotel data path: /app/data/hotels
```

---

## Qu√© ficheros se cargan realmente

Los ficheros que se utilizan como contexto son:

* `hotels.json`
  Informaci√≥n estructurada de los hoteles (nombres, ciudades, precios, etc.).

* `hotel_details.md`
  Texto descriptivo largo con detalles de los hoteles.

Estos ficheros se cargan **una sola vez** y se guardan en memoria.

---

## Cacheo en memoria (comportamiento importante)

Algo que descubr√≠ durante las pruebas es que:

* una vez que el agente carga los datos
* **ya no vuelve a leer los ficheros**
* aunque se modifiquen o se borren en disco

Para comprobarlo:

* mov√≠ `hotels.json` fuera del contenedor
* el agente sigui√≥ respondiendo correctamente
* hasta que reinici√© el contenedor

Tras reiniciar, el agente detect√≥ correctamente que el fichero no exist√≠a y mostr√≥
el error correspondiente.

Conclusi√≥n:
üëâ para que el agente vuelva a leer los datos es necesario reiniciar el proceso.

---

## C√≥mo se construye el prompt

El contexto se env√≠a al modelo **como texto plano**, no como ficheros adjuntos.

El prompt se construye concatenando:

* el contenido completo de `hotel_details.md`
* un `json.dumps` del contenido de `hotels.json`
* la pregunta del usuario

Todo esto se incluye dentro del mensaje de sistema junto con instrucciones
sobre c√≥mo debe responder el asistente.

No hay ning√∫n filtrado ni selecci√≥n previa del contexto.

---

## Flujo completo de una pregunta

1. El usuario escribe una pregunta en la UI web.
2. La pregunta llega al backend por WebSocket.
3. `main.py` llama a `handle_hotel_query_simple`.
4. El agente:

   * carga el contexto (si no est√° en memoria)
   * crea el prompt
   * llama al modelo Gemini
5. La respuesta se devuelve al frontend.

Este flujo se puede seguir f√°cilmente revisando los logs del contenedor.

---

## Configuraci√≥n del modelo

La configuraci√≥n del agente est√° centralizada en:

```
ai_agents_hospitality-api/config/agent_config.yaml
```

En mi caso:

* proveedor: Gemini
* modelo: `gemini-2.5-flash-lite`
* temperatura: 0

La API key se obtiene desde una variable de entorno (`AI_AGENTIC_API_KEY`),
no desde ficheros `.env`.

---

## Pruebas realizadas

### Pruebas manuales

* Preguntas desde la UI web (listado de hoteles, direcciones, precios, etc.).
* Comprobaci√≥n del flujo WebSocket.
* Reinicio del contenedor para probar recarga de datos.
* Prueba de error al faltar `hotels.json`.

### Pruebas automatizadas

* Ejecuci√≥n de `test_exercise_0.py`.
* Todos los tests pasaron correctamente.

---

## Limitaciones detectadas

Este enfoque tiene varias limitaciones claras:

* Se env√≠a todo el contexto en cada pregunta.
* El n√∫mero de tokens crece r√°pidamente.
* No escala para grandes vol√∫menes de datos.
* No hay control sobre qu√© parte del contexto se usa.
* No hay memoria entre preguntas.

Estas limitaciones justifican el uso de RAG en ejercicios posteriores.

---

## Conclusi√≥n

Este ejercicio sirve para entender c√≥mo funciona un agente LLM b√°sico y cu√°les
son los problemas que aparecen cuando se intenta escalar este enfoque.

Es una buena base para introducir t√©cnicas m√°s avanzadas como recuperaci√≥n de
contexto y agentes con herramientas en los siguientes ejercicios.








## üß™ Pruebas y validaciones realizadas

Durante el Exercise 0 se han realizado pruebas manuales y autom√°ticas para validar el funcionamiento del agente simple basado en contexto de ficheros.

### 1) Pruebas unitarias incluidas en el proyecto

Se ejecut√≥ el script `test_exercise_0.py`, que valida el comportamiento b√°sico del agente con distintas preguntas:

```bash
cd ai_agents_hospitality-api
python test_exercise_0.py
````

Pruebas verificadas:

* Listado de hoteles y localizaci√≥n
* Consulta de direcciones
* Planes de comida disponibles
* Informaci√≥n detallada de habitaciones

Resultado:

* ‚úÖ 4/4 tests pasados correctamente

---

### 2) Pruebas manuales desde la UI (WebSocket)

Se prob√≥ el agente desde la interfaz web (`http://localhost:8001`) comprobando el flujo completo:

UI ‚Üí WebSocket ‚Üí agente ‚Üí LLM ‚Üí respuesta

Ejemplos de preguntas probadas:

* *‚ÄúList the hotels in France‚Äù*
* *‚ÄúWhat is the address of Obsidian Tower?‚Äù*
* *‚ÄúWhat meal plans are available?‚Äù*
* *‚ÄúTell me the lowest price for a standard single room in Nice considering no meal plan‚Äù*

En los logs se confirm√≥ el flujo completo:

```txt
Received from ...: {"content":"List the hotels in France",...}
Using Exercise 0 agent for query...
Processing question...
Sent response to ...
```

---

### 3) Verificaci√≥n de carga de datos y rutas

Durante la depuraci√≥n se comprob√≥ qu√© ficheros de datos usa el agente.
Cuando existen, se prioriza la ruta local del contenedor:

```txt
Using local hotel data path: /app/data/hotels
Loading hotel data from /app/data/hotels/hotels.json
```

Tambi√©n se forz√≥ un escenario de error moviendo temporalmente `hotels.json`.
El agente sigui√≥ respondiendo hasta reiniciar el contenedor, confirmando que los datos se cargan una vez y se cachean en memoria.

Tras reiniciar, el servicio detect√≥ correctamente la ausencia de datos y mostr√≥ el aviso correspondiente en logs.

---

### 4) Comandos de depuraci√≥n usados

```bash
docker logs -f ai_agents_hospitality-api
docker exec -it ai_agents_hospitality-api bash
docker restart ai_agents_hospitality-api
```

Estas pruebas confirman que el agente funciona correctamente en el escenario esperado del Exercise 0 y dejan claras sus limitaciones antes de introducir RAG en ejercicios posteriores.

```

